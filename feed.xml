<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://giangson19.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://giangson19.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-23T15:42:53+00:00</updated><id>https://giangson19.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal academic portoflio. </subtitle><entry><title type="html">969 days in the life of a Data Analyst</title><link href="https://giangson19.github.io/blog/969-days-in-the-life-of-a-data-analyst/" rel="alternate" type="text/html" title="969 days in the life of a Data Analyst"/><published>2025-04-10T00:00:00+00:00</published><updated>2025-04-10T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/969-days-in-the-life-of-a-data-analyst</id><content type="html" xml:base="https://giangson19.github.io/blog/969-days-in-the-life-of-a-data-analyst/"><![CDATA[<p style="text-align: justify;">Before my Master's, I worked as a&nbsp;<strong>Data Analyst</strong> at <a href="https://onemount.com/" target="_blank" rel="noopener"><strong>One Mount Group</strong></a> (leading tech ecosystem with 3 digital products) from Oct 2021 until May 2024. I was recruited by the Talent Incubation Program (Fresh Geeks) and was mentored by <a href="https://www.linkedin.com/in/vuhoangt/">Vu Hoang</a> (now PhD Student in Information Systems, CMU). I started in the <a href="https://vinid.net/">VinID</a> analytics team (lifestyle &amp; fintech app, 13+ million users) and then worked simultaneously in the&nbsp;<a href="https://onehousing.vn/">OneHousing</a> team (proptech, 1-2 million monthly traffic) from mid 2022.</p> <center> <img style="width:50%" src="https://media.licdn.com/dms/image/v2/D5622AQHrgyBnsvx2Pw/feedshare-shrink_1280/feedshare-shrink_1280/0/1717167494624?e=1748476800&amp;v=beta&amp;t=Oo5E7MI1usjCr7oPibWOcfJr0_eAI-Y2ZRvPxWUP0sA"/> <p>My employee card. Looks very cool doesn't it?</p> </center> <p style="text-align: justify;">My work at One Mount revolved around three pillars: <strong>Business Intelligence</strong> (analytics &amp; reporting), <strong>Data Engineering&nbsp;</strong>and<strong> Machine Learning</strong>. Some tasks were unconventional for an analyst, due to my tendency to gravitate towards more technical and experimental work.</p> <p style="text-align: justify;">Below is an non-exhaustive list of the projects I contributed to along with what I did and what I learnt. I do my best to describe them without revealing sensitive information.</p> <p style="padding-left: 40px; text-align: justify;"><strong>Machine Learning</strong>:</p> <details style="padding-left: 40px;"> <summary><em>(2023) VinID Customer Income Prediction (xgboost)</em></summary> I leveraged our existing features store and experimented with XGBoost to classify customer into 3 income ranges (multi-class classification). The project was unsuccessful due to the lack of meaningful predictors, and time mismatch between labels (collected in 2019) and features (no data from 2019, so we used data from 2020-2022 as proxy).</details> <details style="padding-left: 40px;"> <summary><em>(2023) VinID Voucher attributes (decision tree)</em></summary> I fitted a decision tree on vouchers with high/low redemption rate and interpreted the tree to identify the most important attributes of a voucher that would affect its redemption.</details> <details style="padding-left: 40px;"> <summary><em>(2022) Onehousing x VinID Lookalike customers (catboost)</em></summary> I used a catboost model (binary classification) to identify customers that are similar to existing homebuyers, using features store from VinID. I also engineered some new features that was considered of high importance by the model. I learnt how to formalize business questions into data science problems, to diagnose the model's performance, and to automate steps in the machine learning pipelines to facilitate experimentations. This was also my first exposure to the imbalanced learning problem.</details> <details style="padding-left: 40px;"> <summary><em>(2022) VinID Notification Interaction Prediction (catboost)</em></summary> I used a catboost model to identify customers that are likely to interact with a notification. I also engineered some new features. I learnt how to quickly experiment with different model configurations and feature combinations.</details> <details style="padding-left: 40px;"> <summary><em>(2021) VinID Winmart holiday sales prediction (Prophet)</em></summary> I attempted to predict 2022 Tet holiday item-level sales using the Facebook's Prophet library. The model was unsuccessful due to the lack of representative data, as the 2021 data was heavily skewed by the COVID-19 pandemic. This is my first exposure to predictive analytics and time series problems.</details> <p style="padding-left: 40px; text-align: justify;">&nbsp;</p> <p style="padding-left: 40px; text-align: justify;"><strong>Analytics Projects:</strong></p> <details style="padding-left: 40px;"> <summary><em>(2024) Onehousing Customer Journey Analysis</em></summary> We analyzed the common paths (each step is a feature on the site) customers took after entering our website. We learnt that there was not a clear common path due to a lack of internal links between pages.</details> <details style="padding-left: 40px;"> <summary><em>(2023) Onehousing Non-Listing Content Problem</em></summary> We explored the behavior of organic users (i.e, they found our website via Google) and attempted to find patterns that would identify high-likelihood house buyers. I led the initiative along with two other analysts, proposed ideas to track the behavior, proposed a metric that corresponded with high retention, and did the early exploratory analysis. I also informed the data tracking template and data warehouse design for this problem.</details> <details style="padding-left: 40px;"> <summary><em>(2022) Onehousing x VinID Growth Project</em></summary> We linked customer attributes (demographic, socio-economic, spatial data, etc.) to real estate purchasing behavior to identify key customer segments. My team and I provided early insights on customer profile informing acquisition strategy, I proposed data collection and experimentation method, and built a dashboard to monitor key project metrics.</details> <p style="padding-left: 40px; text-align: justify;">&nbsp;</p> <p style="padding-left: 40px; text-align: justify;"><strong>Data Engineering</strong>:</p> <details style="padding-left: 40px;"> <summary><em>(2023) Onehousing Alert Engine (Python, SQL, dbt, Airflow)</em></summary> I designed and developed a system that automatically detects mismatched records between two data sources and sends alerts to the Operations and Sales teams. This helps significantly reduce the time spent on data reconciliation. I learnt to thinking in systems.</details> <details style="padding-left: 40px;"> <summary><em>(2023) Onehousing CEO Daily Update Bot (Python, SQL, dbt, Airflow)</em></summary> I built a script that sends daily Slack updates to the CEO about real estate deals in OneHousing. I learnt how to work with the Slack and Tableau API, as well as PyODBC.</details> <details style="padding-left: 40px;"> <summary><em>(2023) VinID Voucher, Notification, Ticketing Datamart (dimensional datawarehouse design)</em></summary> I designed and built dimensional datamarts for the various VinID business functions, which contain data about vouchers, app notification, and ticketing. I learnt a lot about dimensional modelling and data warehouse design in the process.</details> <details style="padding-left: 40px;"> <summary><em>(2022) VinID Data Platform Migration (BigQuery -&gt; Dremio)</em></summary> We changed our data platform and query engine from Bigquery to Dremio. I re-wrote and optimized SQL queries and data pipelines to fit the new platform. I learnt ELT best practices, most of my subsequent pipelines adhered to <a href="https://docs.getdbt.com/best-practices/how-we-style/0-how-we-style-our-dbt-projects" target="_blank" rel="noopener">dbt style guide</a>.</details> <details style="padding-left: 40px;"> <summary><em>All dashboards/reports/models data pipelines (SQL, dbt, Airflow)</em></summary> I built data processing pipelines (partially or entirely) for all projects that I was involved in. I learnt how write readable code and manage my code with Git.</details> <p style="padding-left: 40px; text-align: justify;">&nbsp;</p> <p style="padding-left: 40px; text-align: justify;"><strong>Dashboards:</strong></p> <details style="padding-left: 40px;"> <summary><em> (2024) Onehousing Marketing Dashboard (Power BI)</em></summary> We built executive dashboard for high-level metrics (acqured users, MAU, lead funnel, etc.) of the OneHousing website, with detailed analytical views for specific marketing functions. I learnt how to work with Power BI.&nbsp;</details> <details style="padding-left: 40px;"> <summary><em> (2023) Onehousing Online-to-Offline Dashboard (Tableau)</em></summary> I built an operational dashboard to monitor detailed lead generation and conversion activities by salespeople.</details> <details style="padding-left: 40px;"> <summary><em> (2022) VinID Ticketing Dashboard (Superset)</em></summary> I built an operational dashboard about on-app ticket sales (concerts, football matches, recreational parks etc.) and conversion funnel.</details> <details style="padding-left: 40px;"> <summary><em>(2021) VinID OneView (web-based &amp; Looker Studio)<br/></em></summary> We built a centralized business intelligence platform, containing company-wise key metrics for C-level executives (MTU, MAU, etc.). I built 2 high-level dashboards in 2021: Merchant (about voucher metrics such as claims/redeems) and Product (about north-star app metrics) and I took over as sole maintainer of all related data pipelines in 2022 (200+ tables). I learnt how to debug and track down data errors in a complex pipelines.</details>]]></content><author><name></name></author><category term="work"/><category term="data-analytics,"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[A summary of what I did in my previous industry job.]]></summary></entry><entry><title type="html">Quick introduction to Federated Learning</title><link href="https://giangson19.github.io/blog/quick-introduction-to-federated-learning/" rel="alternate" type="text/html" title="Quick introduction to Federated Learning"/><published>2025-04-05T00:00:00+00:00</published><updated>2025-04-05T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/quick-introduction-to-federated-learning</id><content type="html" xml:base="https://giangson19.github.io/blog/quick-introduction-to-federated-learning/"><![CDATA[<h1 id="centralized-machine-learning">Centralized Machine Learning</h1> <p>In traditional machine learning, all the data is stored in one place and computation all happens on one computer (roughly speaking). The machine learning problem is formalized as minimizing a loss function with respect to the weights.</p> <p>\begin{equation} \min_{w \in \mathbb{R}^d} f(w) \quad \textrm{where} \quad f(w) \overset{\text{def}}{=} \frac{1}{n} \sum_{i=1}^n f_i(w) \end{equation}</p> <ul> <li>$f_i(w)$: Loss (error) on one example, i.e., $\ell(x_i, y_i; w)$</li> <li>$f(w)$: Average loss on predictions made with parameters $w$</li> </ul> <p>And the loss function can be optimized using gradient descent. Each update is:</p> <p>\begin{equation} w_{t+1} \leftarrow w_t - \eta \nabla f(w_t) \end{equation}</p> <ul> <li>$\nabla f(w_t)$: Gradient of the loss function</li> <li>$\eta$: Learning rate</li> </ul> <p>The weights are updated iteratively until convergence or after a certain amount of times.</p> <h1 id="privacy-concerns">Privacy Concerns</h1> <p>As mentioned, to do centralized learning, all the data would need to be sent to one server for compute. This, of course, raises privacy concerns, especially if the training data contains some sensitive information about an user. For example, a keyboard suggestions model would need to collect data on what an user types.</p> <p><a href="https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/">The solution</a> proposed by Google (McMahan et al. 2016) is to distribute the data and the training. Continuing from the keyboard suggestion model, all the data would stay on the user’s phone, and the model training will happen on the device as well. They dubbed their method <strong>Federated Learning</strong>. Since its introduction by Google, some other big companies have used federated learning as well, including <a href="https://oreil.ly/IRKAw" target="_blank" rel="noopener noreferrer">Apple</a>, <a href="https://oreil.ly/YXk1C" target="_blank" rel="noopener noreferrer">NVIDIA</a>, and <a href="https://oreil.ly/e2kGc" target="_blank" rel="noopener noreferrer">Amazon Web Services</a>.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://storage.googleapis.com/gweb-research2023-media/images/1de1662a7e06d2ee886391a458652fcd-F.width-800.format-jpeg.jpg" alt="" width="75%"/> The next parts will outline how to do federated learning.</p> <h1 id="federated-learning">Federated Learning</h1> <p>Federated learning is a procedure where training takes place locally so that there is no need to send data out of the local device. In this process, there will be a centralized server (say, the Google server), and there will be K clients (say K Android users). Learning takes place as follows:</p> <ol> <li>Initialization: At the start of the process, the federated learning server will send a global model (i.e: weights) to the clients. This model could be a pre-trained keyboard suggestion model trained on some public dataset.</li> <li>Local training: The clients will perform training (i.e: gradient descent update steps) using their local data.</li> <li>Transmission: The clients will send the updated weights back to the server.</li> <li>Aggregation: The server will take the weighted average of the received models from clients to be the new global model.</li> </ol> <p>The process is repeated until convergence or after some predefined number of steps. As you can see, only the model weights are sent to the server, the data stays on the users’ phones.</p> <h1 id="local-training-with-fedsgd">Local Training with FedSGD</h1> <p>When the model is trained on a single device, we simply calculate the loss function for that client. And the loss function for the global model is the weighted average of all local losses (*). Intuitively, a client with more data points will contribute more towards the global loss.</p> <p>\begin{equation} f(w) = \sum_{k=1}^{K} \frac{n_k}{n} F_k(w) \quad \text{where} \quad F_k(w) = \frac{1}{n_k} \sum_{i \in \mathcal{P}_k} f_i(w). \end{equation}</p> <ul> <li>$F_k(w)$: Average lost for a single client</li> <li>$f(w)$: Weighted average of $F_k(w)$</li> </ul> <p>When performing gradient descent, the global weights is updated using the weighted average of the local gradients (gradients calculated on local data).</p> <p>\begin{equation} w_{t+1} \leftarrow w_t - \eta \sum_{k=1}^K \frac{n_k}{n} g_k \end{equation}</p> <ul> <li>$g_k$: Average gradient on local data $\nabla F_k(w_t)$</li> </ul> <p>This is done iteratively until… well you get the point.</p> <p>(*) This assumes that data across clients are i.i.d (independently and identically distributed). In practice, it is often the case some some client data are skewed (Zhao et al. 2018), which causes divergence problems. For possible remedy using FedProx, see Li et al. 2020.</p> <h1 id="reducing-communication-rounds-with-fedavg">Reducing Communication Rounds with FedAvg</h1> <p>In the simple FedSGD algorithm, each gradient descent step will incur one round of transmission. Imagine training a model with 100 epochs, that means all clients will have to send (and receive) model weights 100 times. This is inefficient (and for that matter, increases privacy risks that I won’t elaborate here).</p> <p>To reduce the number of communication rounds, we can perform multiple gradient descent updates locally. And afterwards, we use a weighted average of all the local models as the new global model.</p> <p>\begin{equation} w_{t+1}^{k} = w_t - \eta g_k \end{equation}</p> <p>\begin{equation} w_{t+1} \leftarrow \sum_{k=1}^K \frac{n_k}{n} w_{t+1}^{k} \end{equation}</p> <ul> <li>$w_{t+1}^{k}$: Local weight for client $k$ in iteration $t+1$</li> </ul> <p>Notice that with FedSGD, the <strong>local gradients</strong> are sent back to the server, whereas in FedAvg, the <strong>local weights</strong> (updated after several epochs) are sent instead. Moreover, the gradients are only transmitted after some number of epochs (as opposed to every epoch), thus reducing the number of communication rounds.</p> <h1 id="challenges">Challenges</h1> <p>While federated learning reduces privacy risks by keeping the data local while training the model instead of sending it to the server, it is still vulnerable to some other types of attack such as gradient inversion attack (Geiping et al. 2020) or membership inference attack. I will leave the of elaboration these risks as well as potential defenses to my future self as homework. (It literally is my homework, due in about 5 days).</p> <h1 id="references">References</h1> <p>[1] McMahan, B., Moore, E., Ramage, D., Hampson, S. &amp; Arcas, B.A.y.. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em>, in <em>Proceedings of Machine Learning Research</em> 54:1273-1282</p> <p>[2] Geiping, J., Bauermeister, H., Dröge, H., &amp; Moeller, M. (2020). Inverting gradients-how easy is it to break privacy in federated learning?. <em>Advances in neural information processing systems</em>, <em>33</em>, 16937-16947.</p> <p>[3] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., &amp; Chandra, V. (2018). Federated learning with non-iid data. <em>arXiv preprint arXiv:1806.00582</em>. </p> <p>[4] Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., &amp; Smith, V. (2020). Federated optimization in heterogeneous networks. <em>Proceedings of Machine learning and systems</em>, <em>2</em>, 429-450. </p> <p>[5] <a href="https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/" target="_blank" rel="noopener">Federated Learning: Collaborative Machine Learning without Centralized Training Data</a> </p>]]></content><author><name></name></author><category term="study"/><category term="data-science,"/><category term="machine-learning,"/><category term="data-privacy,"/><category term="deep-learning"/><summary type="html"><![CDATA[A short technical introduction to federated learning, a framework for privacy preserving machine learning.]]></summary></entry><entry><title type="html">What’s it like to study in a Master’s?</title><link href="https://giangson19.github.io/blog/whats-it-like-to-study-in-a-masters/" rel="alternate" type="text/html" title="What’s it like to study in a Master’s?"/><published>2025-03-15T00:00:00+00:00</published><updated>2025-03-15T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/whats-it-like-to-study-in-a-masters</id><content type="html" xml:base="https://giangson19.github.io/blog/whats-it-like-to-study-in-a-masters/"><![CDATA[<table> <tbody> <tr> <td>Posted by Giang Son</td> <td>Mar 15, 2025</td> <td>2 min read Quick notes on how studying in a Master’s differs from other levelsIn my humble experience, and roughly speaking.Sauce: The illustrated guide to a Ph.D.(*) which is why I think it is best to have some prior working experience before entering a Master’s. I believe you would gain a lot more compared to going in blind. Thank you for reading. I’ve also written some other posts that you can check out. Copyright © 2023 Giang Son. Meticulously handcrafted with Django and Bootstrap.</td> </tr> </tbody> </table>]]></content><author><name></name></author><summary type="html"><![CDATA[Quick notes on how studying in a Master's differs from other levels]]></summary></entry><entry><title type="html">Gentle introduction to Linear Regression</title><link href="https://giangson19.github.io/blog/gentle-introduction-to-linear-regression/" rel="alternate" type="text/html" title="Gentle introduction to Linear Regression"/><published>2025-03-01T00:00:00+00:00</published><updated>2025-03-01T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/gentle-introduction-to-linear-regression</id><content type="html" xml:base="https://giangson19.github.io/blog/gentle-introduction-to-linear-regression/"><![CDATA[<h1>The problem</h1> <p>Consider this problem: You are looking to buy a new house. You know some information about the property: the area of each floor, the number of bedrooms,&hellip; Now, you want to know how much you should be paying for that building. <em>How would you go about estimating its price?</em></p> <p>You could, in theory, go to some real estate guru and they would tell you, based on their knowledge and experience and whatnot, how much they think the house is worth. However, this manual approach may have some downsides: an expert&rsquo;s estimate could be costly, slow and biased / subjective.</p> <p>Now, suppose you already have some historical data about other houses and their prices, then you can turn to machine learning to get a cheap and <em>unbiased</em> estimate.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear_regression/lr_data_table-480.webp 480w,/assets/img/posts/linear_regression/lr_data_table-800.webp 800w,/assets/img/posts/linear_regression/lr_data_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/linear_regression/lr_data_table.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1>The data</h1> <p>(The dataset I used as example can be downloaded from Kaggle <a href="https://www.kaggle.com/datasets/lespin/house-prices-dataset">at this link</a>).</p> <p>Let&rsquo;s take a look at our dataset. The data we have is stored in a table, made up of 1460 rows and 6 columns. Each row represent a &ldquo;<em>data point</em>&rdquo; or &ldquo;<em>example</em>&rdquo; (in this case, each row is a house), and the columns are information we have on example. The column SalePrice is the value that we are trying to predict for future houses, it is usually called the &ldquo;<em>target"</em>&nbsp;or the &ldquo;<em>label</em>&rdquo;. The rest of the columns are information that we will use to make our prediction, and they are called &ldquo;<em>features</em>&rdquo;.</p> <p>For your convenience, I shall describe each of the feature below. When the data is structured (it comes in a table) and each feature has its own meaning, it is often helpful to understand the features and how it relates to our target.</p> <ul> <li><strong>OverallQual</strong>: Overall material and finish quality (from 1 to 10)</li> <li><span class="sc-jhZTHU dVJzog"><strong>BedroomAbvGr</strong>: Bedrooms above grade&nbsp;</span></li> <li><strong>GarageCars</strong>: Size of garage in car capacity</li> <li><strong>FullBath</strong>: Full bathrooms above grade</li> <li><strong>GrLivArea</strong>: Above grade (ground) living area square feet</li> </ul> <p>By machine learning convention, we usually divide the dataset into two parts: 80% of the rows will be used for training, and 20% will be used for testing. (We reserve the 20% testing set to simulate the fact that real world, our model will make predictions on data it has not seen before - we shall see how it is used to evaluate our model in a later section). We denote the number of examples used for training as \(N\), and the number of features as \(P\). In this case, \(P = 5\) and \(N = 1168\).</p> <p>Now if you squint your eyes hard enough, this table sort of looks like&hellip; a matrix. For the features, it is a matrix of \(N\) rows and \(P\) columns. For the label, it is a matrix of \(N\) rows and \(1\) column (equivalently, a column vector). We call the matrix for the features \(X\), and the label vector \(y\). (The \(y\) is in lowercase because it is a vector, whereas \(X\) is a matrix). Formally, \(X \in \mathbb{R} ^ {N \times P}\) and \(y \in \mathbb{R} ^{P}\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear_regression/lr_data_table_matrix-480.webp 480w,/assets/img/posts/linear_regression/lr_data_table_matrix-800.webp 800w,/assets/img/posts/linear_regression/lr_data_table_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/linear_regression/lr_data_table_matrix.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And here is the matrices written out explicitly:</p> <p style="text-align: center;">\( \mathbf{X} = \begin{bmatrix} 7 &amp; 3 &amp; 2 &amp; 2 &amp; 1710 \\ 6 &amp; 3 &amp; 2 &amp; 2 &amp; 1262 \\ 7 &amp; 3 &amp; 2 &amp; 2 &amp; 1786 \\ 7 &amp; 3 &amp; 3 &amp; 1 &amp; 1717 \\ 8 &amp; 4 &amp; 3 &amp; 2 &amp; 2198 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 6 &amp; 3 &amp; 2 &amp; 2 &amp; 1647 \\ 6 &amp; 3 &amp; 2 &amp; 2 &amp; 2073 \\ 7 &amp; 4 &amp; 1 &amp; 2 &amp; 2340 \\ 5 &amp; 2 &amp; 1 &amp; 1 &amp; 1078 \\ 5 &amp; 3 &amp; 1 &amp; 1 &amp; 1256 \end{bmatrix} \)</p> <p style="text-align: center;">\( y = \begin{bmatrix} 208500 \\ 181500 \\ 223500 \\ 140000 \\ 250000 \\ \vdots \\ 175000 \\ 210000 \\ 266500 \\ 142125 \\ 147500 \end{bmatrix} \)</p> <p style="text-align: left;">Writing a column vector be burdensome, so sometimes we can write one as a row vector than add a little \(^T\) (meaning "transposed") at the end, like this:&nbsp;</p> <p style="text-align: left;">\( y = [208500, 181500, ..., 142125, 147500]^T \)</p> <p>To add just a little bit more notation: we shall use superscript numbers \(^{(i)}\) to denote the \(i^{th}\) row, and the subscript number \(_m\) to denote the \(m^{th}\) column. For example, you can see that \(x^{(1)} = [7, 3, 2, 2, 1710]\) (the first row), \(x_1 = [7, 6, 7, 7, 8, &hellip;]\) (the first column OverallQual) and \(x^{(1)}_5 = 1710\).</p> <h1>The model</h1> <p>The premise of machine learning is this: there exists a model (sort of a math function) that can accurately represent the relationship between the features and the target for each training example (also for unseen data), which we shall estimate from the data. Formally: give N training examples (\(x^{(1)}, y^{(1)}\)), (\(x^{(2)}, y^{(2)}\)), &hellip; (\(x^{(N)}, y^{(N)}\)), we aim to learn a mapping \(f: x &rarr; y\) by requiring \(f(x^{(i)}) = y^{(i)}\); such that for any unseen \(x^*\)<em>, </em>\(f(x^*) = y^*\) (or something along those lines).</p> <p>We start by defining a model that represents the relationship between X and y. And by &ldquo;defining&rdquo; I mean literally guessing because a model is, more or less, our assumption of how X and y is related; and we can try out different models to see which one most accurately captures how X affects y.</p> <p>Say, we hypothesize that the price of a house has a linear relationship with the features OverallQual, BedroomAvgGr,&hellip; A linear relationship is one where when OverallQual increases by 1, then SalePrice will always increases by some fixed amount.&nbsp;</p> <p>We now have a model called <strong>Linear Regression</strong>. &ldquo;Regression&rdquo; is when our target takes continuous values (which SalePrice does), and &ldquo;Linear&rdquo; because as you see, \(y\) (SalePrice) is a linear combination of \(x\). This is the simplest model that you can come up with in machine learning.</p> <p>The functional form looks like this:</p> <p style="text-align: center;">\( f(x) = \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ...+ \beta_P \cdot x_P + \beta_0 \cdot 1 \)</p> <p>where \(\beta_0\) to \(\beta_P\) (pronounced &ldquo;beta&rdquo;) are &ldquo;learnable&rdquo; parameters (because there value will be &ldquo;learnt&rdquo; from the data, we shall see this later). \(\beta_0\) in particular is a thing called the &ldquo;bias term&rdquo; (or the &ldquo;intercept&rdquo; in statistics), but I will pretend like it&rsquo;s just another parameter like the other \(\beta\), because it is estimated in the exact same way.</p> <p>Using vector notation, we can succinctly write the model as:</p> <p style="text-align: center;">\( f(x) = X \cdot \beta \)</p> <p>where \(X\) is the feature matrix \(X = [x_1, x_2, &hellip;, x_p]\) (each \(x_m\) is a column as we have seen above) and \(\beta\) is a column vector \( \beta = [\beta_1, \beta_2, &hellip;, \beta_p]^T\). You may verify that the dot product operation \(X \cdot \beta \) indeed results in the linear combination in the previous formula.</p> <p>To add just one more bit of notation: after we have obtained \(\beta\), we can plug its values into the model and get some prediction for SalePrice, but its output will be slightly different from the true SalePrice. We call the predictions made by the model \(\hat{y}\) (pronounced y-hat). From now on, we will use the hat notation to denote anything that is estimated from data, and not ground truth data itself.</p> <p>We can write:</p> <p style="text-align: center;">\( \hat{y} = f(x) = X \cdot \beta \)</p> <p>Now, how do we train the model to learn \(\beta\)?</p> <h1>The learning</h1> <p>As we mentioned above, the objective of machine learning is to estimate a model that give an accurate approximate of some \(y^*\)<em>&nbsp;</em>when given some new data \(x^*\). In Linear Regression, we assume that the existing data and new data follow the same distribution and that the two don't affect each other's value (the "identically and independently distributed" or i.i.d assumption). In other words, we assume that when the model makes good predictions on existing data, it will also make good predictions on new data.</p> <p>How do we "train" the model to make good prediction for \(y\) based on existing data \(x\)? We can quantify the goodness of our model by measuring the error, i.e: how wrong the model is, using something called the "loss function".&nbsp; By minimizing the loss function, we get a model that is most accurate on existing data.&nbsp;</p> <p>Just bear with me for a minute to see how to minimize this and what this has to do with learning \(\beta\).</p> <p>In Linear Regression, the popular loss function to use is the Mean Squared Error (MSE):&nbsp;</p> <p style="text-align: center;">\( MSE = \frac{1}{n}\displaystyle\sum_{i=1}^N(\hat{y}^{(i)} - y^{(i)})^2 \)</p> <p>This loss function does exactly what it name says:</p> <ul> <li>for each training example, <ul> <li>measure how wrong the prediction is by subtracting the prediction from the true value, (error)</li> <li>square that up so negative difference is the same as positive difference (e.g: if the true value is 100, then a prediction of 97 is just as wrong as a prediction of 103), (squared)</li> </ul> </li> <li>take the average over all training examples (mean)</li> </ul> <p>In vector form, the loss function can be written like this: (again, feel free to verify that it is the same operation):</p> <p style="text-align: center;">\( MSE = \frac{1}{n}(\hat{y}-y)^T(\hat{y}-y) \)</p> <p>where \(\hat{y}\) and \(y\) are both column vectors, \(\hat{y} - y\) is therefore a column vector and \((\hat{y} - y)^T\) is the transpose of that vector. This dot product operation will result in squares of each element in \(\hat{y}\) and \(y\).</p> <p>Now, we can substitute \(\hat{y} = X \cdot \beta\) into this and get</p> <p style="text-align: center;">\( MSE = \frac{1}{n}(X \beta - y)^T(X\beta - y) \)</p> <p>In this function, we already know \(X\) and \(y\) (that is our given data), so the only unknown variable is \(\beta\). Our objective now is to find the values for \(\beta\) that will minimize MSE. Our machine learning problem (and most ML models) can be viewed as a function optimization problem (i.e: we are always trying to minimize some loss function).</p> <p><span style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;">This particular loss function for linear regression is convex, which we can find a closed form solution to this minimization problem. We obtain the solution by setting the partial derivative of function with respect to \(\beta\) to \(0\) and solve for \(\beta\). Formally:</span></p> <p style="text-align: center;">\( \frac{\delta \frac{1}{n}(X \beta - y)^T(X\beta - y)}{\delta{\beta}} = 0 \)</p> <p>Using vector calculus magic we can get the left hand side of the equation to the following. I will leave out the step by step details because it&rsquo;s quite complicated, so comment or DM me if you want to know how to actually derive this.</p> <p style="text-align: center;">\( X^TX\beta - X^Ty = 0 \)</p> <p>Solving for \(\beta\) we will get: (*)</p> <p style="text-align: center;">\( \beta = (X^TX)^{-1}X^Ty \) (**)</p> <p>This, ladies and gentlemen<strong>, is our machine learning model.</strong></p> <p>(*) The method we are using is equivalent to the Ordinary Least Squares method in Statistics.</p> <p>(**) The implicit implication in writing \((X^TX)^{-1}\) is that the matrix \( X^TX \) has an inverse. The informed readers may realize not all matrices are invertible. However, dealing with this is another topic I won't cover in this post.</p> <h1>Evaluation</h1> <p>That last part was quite a handful, especially if you are not too familiar with linear algebra and calculus. Don&rsquo;t worry though, in my class of over 100 people, I can see only one single guy that can follow along this particular lecture (or any lecture in Machine Learning, for that matter).</p> <p>What I want you to take away is this: our machine learning model is, in the end, obtained a bunch of matrix multiplication involving \(X\) and \(y\), which is our original data table.</p> <p>And you might ask yourself: How does that even work? Like, how does such a model have any predictive power?</p> <p>Surprisingly, it does kinda work. And even more surprisingly, lots of AI models kinda work in the same way.</p> <p>If you download the dataset and try to calculate the parameters in beta for yourself (can be done using Excel if you are not familiar with Python), then you will get the following results :</p> <p>\(\displaylines { \beta_1 = 25111.14712806\\ \beta_2 = -9149.25660938\\ \beta_3 = 21069.61456549\\ \beta_4 = 1927.11001388\\ \beta_5 = 56.44300459\\ \beta_0 =-72622.68631023428 }\)</p> <p>We can make prediction to obtain \(\hat{y}\) for the 20% we reserved in the beginning. To evaluate its accuracy, we can use the Root Mean Squared Error (the square root of the above MSE - we are using the square root because it is in the same unit as our target variable). In this case:</p> <p>\(RMSE = 41832.23667930187\)</p> <p>So when we use this model to estimate house prices, its output is, on average, about \$41k off the actual price. Considering that the average price is around \$180k, this is not completely terrible.</p> <p>We can examine this in a bit more details by plotting the Predicted Price and the Actual Price. The closer the points are to the red line, the smaller the error. If a model is 100% accurate, all the dots will be on the red line, hence error will be 0. You can see that the model does pretty well for houses worth below $350k, it is the extremely expensive houses (400k and above) that the model struggles with.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear_regression/lr_actual_vs_predicted-480.webp 480w,/assets/img/posts/linear_regression/lr_actual_vs_predicted-800.webp 800w,/assets/img/posts/linear_regression/lr_actual_vs_predicted-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/linear_regression/lr_actual_vs_predicted.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And the effectiveness of this model can be due to a few reasons:</p> <ul> <li>First, the model we chose (linear regression) is rather a decent choice for a model because lots of variables in the real world follow a linear relationship. Not all relationships are linear, but a lot.</li> <li>Second, the method that we solved for the model&rsquo;s parameters practically ensures that the predictions, on average, are as close to the real values as possible.&nbsp;</li> </ul> <p>There are ways to improve the model and drive down the error. Namely, we can choose a more complex model (say, one that can captures non-linear relationships). Or we can incorporate more features into our model (the dataset has some 80+ features, I only used 5 to keep things simple). Or better yet, we do proper feature engineering and data preparation (which is where Data Science comes in). For instance, if I use a Random Forest models along with some clever feature engineering (combining some features to create a new one), I can get the error to as low as ~23k, which is quite the improvement compare to ~41k.</p> <h1>Conclusion</h1> <p>Well well, congratulations for making it to the end of the post. By now I hope you get a taste of what machine learning is about. Even though the model we used in this post is quite simple, it does provide some predictive power, and more importantly, it provides a basis for us to learn more complex models. Essentially, a lot of machine learning modeling can be simplified to the steps that we have followed here:</p> <ul> <li>Collect some data</li> <li>Specify a model</li> <li>Define a loss function</li> <li>Find the parameters that minimize the loss function</li> </ul> <p>And even some of the math that we did here will come up again in other models, assuming that I will continue writing on machine learning. Do drop a comment if you want to see more.</p> <p>Until then, you may find the Python code for this model <a href="https://github.com/giangson19/gentle_ml/blob/main/linear_regression_housing_prices.ipynb">at this link</a> (for those unfamiliar with Python, please comment if you want some explanation).</p>]]></content><author><name></name></author><category term="study"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[A gentle introduction to the simplest machine learning problem, linear regression.]]></summary></entry><entry><title type="html">My tutoring experience</title><link href="https://giangson19.github.io/blog/my-tutoring-experience/" rel="alternate" type="text/html" title="My tutoring experience"/><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/my-tutoring-experience</id><content type="html" xml:base="https://giangson19.github.io/blog/my-tutoring-experience/"><![CDATA[<p style="text-align: justify;">From Sep to Nov 2024, I was a <strong>Data Analytics Mentor</strong> (teaching assistant) at <strong>MindX Technology School</strong>. I mentored a group of 10 adult learners in Python, SQL and Power BI lessons. I expanded on concepts taught by the main teacher, guided students through technical exercises and provided them with practical advice on pursuing a career in data.&nbsp;</p> <p style="text-align: justify;">Between Dec 2022 and May 2024, I was a <strong>Data Analytics Mentor</strong> (internal trainer) at <strong>One Mount</strong>. I built a custom curriculum and taught a 5-lesson introductory SQL course for 30+ colleagues from the Marketing and Business Development department in Dec 2022. I gave practical advice on querying large datasets (billions of rows) and analytical methods specific to our business context.</p> <center><img src="https://media.licdn.com/dms/image/v2/D5622AQHiCYDSnAAw3Q/feedshare-shrink_800/feedshare-shrink_800/0/1717167494327?e=1748476800&amp;v=beta&amp;t=kwEyRJsasOTn7NBTStZfkjZhLauHeB_w26-bBSDy6jk" style="width:30%;"/></center> <p></p> <p style="text-align: justify;">Within my data analytics team of 10+ (our population varied over the years), I led multiple training sessions on technical skills relevant to our projects: applied machine learning, machine learning explanability, inferential statistics, dimensional data warehouse design (most of which I self-learnt). Ocasionally, I shared my knowledge about soft skills, namely critical thinking and problem solving.</p> <p style="text-align: justify;">From Mar to Nov 2019, I was an <strong>IELTS Tutor</strong> (teaching assistant) at <strong>LinhUK IELTS School</strong>. I assisted in classes of 4 IELTS skills, working mostly with high-school and university students. I graded writing assignments and provided detailed feedback on how to improve grammatical structure, lexical usage, and idea development. I also trained individually with numerous students in speaking exercise, giving practical advice on pronounciation, intonation and building reflexes to test questions.</p>]]></content><author><name></name></author><category term="work"/><category term="data-analytics,"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[Looking back at the times when I shared my knowledge]]></summary></entry></feed>