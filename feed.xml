<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://giangson19.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://giangson19.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-03T08:56:10+00:00</updated><id>https://giangson19.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal academic portoflio. </subtitle><entry><title type="html">Renting GPUs to do homework</title><link href="https://giangson19.github.io/blog/renting-gpus-to-do-homework/" rel="alternate" type="text/html" title="Renting GPUs to do homework"/><published>2025-05-03T00:00:00+00:00</published><updated>2025-05-03T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/renting-gpus-to-do-homework</id><content type="html" xml:base="https://giangson19.github.io/blog/renting-gpus-to-do-homework/"><![CDATA[<blockquote> <p>Disclaimer: Renting GPUs, at least the way described in this blog post, is not totally secured, nor is it the only choice, nor are my practices best practices. Readers can feel free to suggest better methods, I am in need of them too. I also assume your know some Linux commands (like <code class="language-plaintext highlighter-rouge">cd</code>) and when to use them.</p> </blockquote> <h1 id="why-rent-gpu">Why rent GPU</h1> <p>In my last semester at NTU, I had to do some work with deep learning, including both model training and inference. It needed the power of the GPU, but I only have measly Macbook Air. My options include: to build a PC with GPU (too expensive, although I want one down the line so I can play PC games too), to use the school’s provided GPU (too much of a hassle since I’m not familiar with the procedure, also I don’t think it is that powerful), to use Google Colab (slow, also not enough VRAM), or to rent a GPU for a few hours via some online service. The last option is the most convenient and quite price effective, as we shall see below.</p> <h1 id="where-to-rent-and-pricing">Where to rent and pricing</h1> <p>You can go on Google and search for “rent GPU” and it will return plenty of sites. I personally used <a href="http://vast.ai">vast.ai</a> (not affiliated, just the first site that came up in my search). Go to their website and you will see plenty of options: you can choose from a wide range of GPU and storage options, with different pricing of course. I can go as low as RTX 3090 (~0.2$/hr) or as high as A100 (~0.9$/hr), which I think is not too expensive – my last group project costed about 10$ in total to train on an A100. Choosing a GPU is matter of need and experience, and unfortunately in this issue I have no advice – I am just as confused as the next guy.</p> <h1 id="caveats">Caveats</h1> <p>If you use your own machine, or Colab for that matter, than you’d be used to having everything set up from the get go (machine learning packages already installed, etc.). This is not the case when renting GPU. Everytime you rent a new GPU, you’d have to configure git, and set up the Python environment, etc. When you rent GPUs, you only rent them temporarily (for a few hours a day), so if you have to work over the course of a few days, chances are you’d have to repeat the setting up process several times. That’s also why I wrote this post, to share some tricks that will speedup the process.</p> <p>Not to mention, you’d also have to re-download your data onto the new machine, which can be quite the hassle if it’s several GBs. For this I can offer no solutions, other than maybe choose a machine with fast internet speed (on vastai you can see the internet speed of the machine).</p> <p>On top of all this, as far as I know renting a GPU is not totally secured, since you’d be putting your data on what is essentially a stranger’s machine. I’m not a security expert but who knows what can happen, so it’s best keep the use cases to just homework or personal projects.</p> <h1 id="getting-started">Getting started</h1> <p>To rent a GPU is quite simple, the UI is pretty much self-explanatory so you can just go on the website, choose which GPU you want, and press RENT.</p> <h1 id="interacting-with-the-gpu">Interacting with the GPU</h1> <p>For a more comprehensive guide, do check out the documentation of your chosen service provider. I’ll only summarize a few of my practices here. (That said, I find vastai documentation to be quite confusing to a beginner).</p> <p>As far as I understand there are two ways to use the GPU.</p> <p>Vastai provides a Jupyter Lab / Jupyter Notebook interface plus a terminal, so you can just use that. It looks exactly the same as using a Jupyter Notebook on your local machine. If you want to go with this option you won’t need the rest of this post.</p> <p>The second option, which I prefer for several reasons, is that you can connect to the virtual machine using SSH (from now on let’s just call the machine with GPU the “virtual machine”, I don’t have a better terminology). This has the added benefit of being able to use your own terminal and code editor (I am quite picky when it comes to the choice of editor), which I shall explain below.</p> <h1 id="ssh-keys">SSH Keys</h1> <p>To connect via SSH, you’d need to first create a public/private key pair on your local machine. You’d provide the public key to the virtual machine, so that when your local machine connects, the virtual machine knows that it’s you. You may have done this when setting up your GitHub account. I <em>feel</em> like it’s better to have a separate key pair for the GPU renting purpose (although I can’t quite explain why).</p> <p>On your local terminal run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-t</span> rsa
</code></pre></div></div> <p>And then type in the directory in which to save your keys and press enter. For me it looks something like <code class="language-plaintext highlighter-rouge">/Users/giangson/.ssh/id_rsa_gpu</code>. If you don’t provide a directory it will save in the default path which is <code class="language-plaintext highlighter-rouge">/Users/giangson/.ssh/id_rsa</code>, which I already used for my GitHub.</p> <p>To get the content of the public key, run: (do change out my path with the path on your machine)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> /Users/giangson/.ssh/id_rsa_gpu
</code></pre></div></div> <p>This will output the public key which looks something like this: (not my actual private key btw, never share your private key)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAZBAQDdxWwxwN5Lz7ubkMrxM57CHhVzOnZuLt5FHi7J8zFXCJHfr96w+ccBOBo2rtBBTTRDLnJjIsKLgBcC3+jGyZhpUNMFRVIJ7MeqdEHgHFvUUV/uBkb7RjbyyFcb4BCSYNggUZkMNNoNgEa3aqtBSzt47bnuGqKszs9bfACaPFtr9Wo0b8p4IYil/gfOY5kuSVwkqrBCWrg53/+T2rAk/02mWNHXyBktJAu1q9qTWcyO68JTDd0sa+4apSu+CsJMBJs3FcDDRAl3bcpiKwRbCkQ+N63ol4xDV3zQRebUc98CJPh04Gnc41W02lmdqFL2XG5U/rV8/JM7CawKiIz3dbkv bob@velocity
</code></pre></div></div> <p>You need to copy this whole thing (beginning from ssh-rsa to the email at the end).</p> <p>With this public key you can do two things.</p> <p>First, you can go vastai and add this key to the virtual machine so that it can authenticate you when connect (once you’ve rented a machine there’s a button to add SSH keys).</p> <p>Second, you can go to GitHub and add this to your list of SSH keys, i.e you’ll also use this public-private keypair to authenticate with GitHub.</p> <h1 id="ssh-connect">SSH Connect</h1> <p>Once you’ve added the public key to the machine, you’ll see this command pop up:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/renting_gpus/ssh_command-480.webp 480w,/assets/img/posts/renting_gpus/ssh_command-800.webp 800w,/assets/img/posts/renting_gpus/ssh_command-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/renting_gpus/ssh_command.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This commands means that you’re connect (<code class="language-plaintext highlighter-rouge">ssh</code>) to the machine at IP <code class="language-plaintext highlighter-rouge">5.17.27.63</code> as <code class="language-plaintext highlighter-rouge">root</code> user through port <code class="language-plaintext highlighter-rouge">46769</code>. If you were to run this exact command I don’t think it’ll work, because you don’t have the my private key (that’s why we set up the key pair in the previous step).</p> <p>Anyway, once you have the command specific to your machine, run it in the terminal and you’ll be connected to the VM.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/renting_gpus/ssh_connected-480.webp 480w,/assets/img/posts/renting_gpus/ssh_connected-800.webp 800w,/assets/img/posts/renting_gpus/ssh_connected-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/renting_gpus/ssh_connected.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="setting-up-git-and-the-environment">Setting up Git and the environment</h1> <p>In the previous step we have added a public key to your GitHub account, now to use it on the VM it’d need to have the corresponding private key.</p> <p>You can get the private key like this by running it in your local terminal</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> ~/.ssh/id_rsa_gpu
</code></pre></div></div> <p>Same deal as before, copy all the content.</p> <p>Then add it on the VM:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">' &gt; ~/.ssh/id_rsa
(add your private key here)
</span><span class="no">EOF

</span><span class="nb">chmod </span>600 ~/.ssh/id_rsa
<span class="nb">chmod </span>700 ~/.ssh

</code></pre></div></div> <p>Alternatively, you can create another public-private key pair on the VM. I find this to be quite inconvenient, although it may be a bit more secured? The reason is that with each new key pair, you need to add the new public key to your GitHub which can take a few steps, whereas if you’re reusing the old key pair then you’d only need to add that once.</p> <p>Anyway, now that you’re set up to use git commands. You can start cloning your repo onto the VM.</p> <p>To push, you’d also need to run the following commands on the VM: (remember to change out the values with yours)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">eval</span> <span class="sb">`</span>ssh-agent <span class="nt">-s</span><span class="sb">`</span>
ssh-add ~/.ssh/id_rsa

<span class="c"># Avoid GitHub authenticity prompt</span>
<span class="nb">echo</span> <span class="s2">"🌐 Adding GitHub to known_hosts..."</span>
ssh-keyscan github.com <span class="o">&gt;&gt;</span> ~/.ssh/known_hosts
<span class="nb">chmod </span>644 ~/.ssh/known_hosts

<span class="c"># Optional: Configure Git identity (needed for pushing)</span>
git config <span class="nt">--global</span> user.name <span class="s2">"your name"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"your_email@example.com"</span>
</code></pre></div></div> <h1 id="your-repo">Your repo</h1> <p>One thing that I haven’t mentioned is that it is better to have your code written before renting the GPU and after renting you’d need to just execute it (to save time and money). Most likely you’ve written your machine learning code and pushed it to your repo. Even better if you have some code to automatically load data (which is already the case when you’re working with generic datasets, i.e: MNIST, CIFAR and such).</p> <p>It is even better if in your repo there’s a <code class="language-plaintext highlighter-rouge">requirements.txt</code> that contains all the necessary packages to run your program. I’ve seen some people not include this? I mean if you want to go manually write our your pip install command then sure but I think a requirements file is just much better practice (we’ll see below).</p> <p>To create the <code class="language-plaintext highlighter-rouge">requirements.txt</code> file you can either run this command on your local environment</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip freeze <span class="o">&gt;</span> requirements.txt
</code></pre></div></div> <p>Which will automatically lists all of your installed packages and their versions into a txt file. This is quite handy, but for my taste I’d often just list out the packages I know I need for one project (because I prefer a minimalistic approach). This is quite easy, I just look at my import statements.</p> <p>So my requirements file often just looks like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch
datasets
torchvision
numpy
pandas
matplotlib
scikit-learn
transformers
tqdm
tokenizers
rouge_score
ipython
ipykernel
</code></pre></div></div> <p>The purpose a requirements file is that you can quickly install all the package with this command <code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code>.</p> <p>All in all, after you’re set up the private-key pair, you can run the following commands to clone your codebase, set up a virtual environment and install the packages: (also be sure to change out the repo and folder name)</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"📂 Cloning GitHub repository..."</span>
<span class="nv">GITHUB_REPO</span><span class="o">=</span><span class="s2">"git@github.com:giangson19/DeepLearningLLM.git"</span>
git clone <span class="s2">"</span><span class="nv">$GITHUB_REPO</span><span class="s2">"</span>
<span class="nv">REPO_DIR</span><span class="o">=</span><span class="s2">"DeepLearningLLM"</span> <span class="c"># Folder name after cloning</span>
<span class="nb">cd</span> <span class="s2">"</span><span class="nv">$REPO_DIR</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">"🐍 Setting up Python virtual environment..."</span>
<span class="nv">VENV_NAME</span><span class="o">=</span><span class="s2">"deeplearning"</span>                         <span class="c"># Virtual environment name</span>
<span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="s2">"python3.8.5"</span>               <span class="c"># Change if needed</span>
<span class="nv">$PYTHON_VERSION</span> <span class="nt">-m</span> venv <span class="s2">"</span><span class="nv">$VENV_NAME</span><span class="s2">"</span>
<span class="nb">source</span> <span class="s2">"</span><span class="nv">$VENV_NAME</span><span class="s2">/bin/activate"</span>

<span class="nb">echo</span> <span class="s2">"📦 Installing Python dependencies..."</span>
pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <h1 id="vscode-remote">VSCode Remote</h1> <p>If you’re a pro then from the previous step you’re already set up to run your program in the terminal like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python run_weight_pruning.py
</code></pre></div></div> <p>But I sometimes need some final code edits or just a nice UI for file organization. VSCode, my editor of choice, has this handy extension called <a href="https://marketplace.visualstudio.com/items/?itemName=ms-vscode-remote.vscode-remote-extensionpack">Remote Development</a>. Basically it lets you SSH into the VM, so that you can write code in VSCode and execute it in the VM with GPU. This means I can use all the features provided by my beloved code editor (dark theme, code linting, code generation with CoPilot, etc.). Basically my development experience will remain the same (it can be jarring to switch between different editors, since my muscle memory is already tied to VSCode).</p> <p>You can install the extension via the above link. To connect, open command palette with Ctrl-Shift-P (or Cmd-Shift-P on Mac, this is one of the muscle memory thing I was talking about), type and choose Remote-SSH: Add New SSH Host → copy and paste in the SSH string you’ve got from a previous step → choose the directory to save config (just choose the default).</p> <p>Once this is done, VSCode will have a pop up asking you to connect (just press connect). Otherwise, Ctrl-Shift-P and choose Remote-SSH: Connect to SSH Host and choose the VM you’ve just added.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/renting_gpus/ssh_connect_vscode-480.webp 480w,/assets/img/posts/renting_gpus/ssh_connect_vscode-800.webp 800w,/assets/img/posts/renting_gpus/ssh_connect_vscode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/renting_gpus/ssh_connect_vscode.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And you’re ready to code. If you’re already familiar with VSCode then I think you can find your way from here on in.</p> <h1 id="tmux">Tmux</h1> <p>One final trick that I’ve just recently learn is Tmux. If you’ve used Colab then you know that as soon as you’re disconnected (say, your laptop goes to sleep), then your programm stops running and you’ll have to restart – which can be quite frustrating. Same goes for GPU renting.</p> <p>To avoid this, you can use Tmux. At its most simple, Tmux creates a persistent terminal session that will keep running even if your local machine is temporarily disconnects from the VM.</p> <p>By default, the VMs on vastai start in a tmux session. But if your some reason it doesn’t (or you’re like me and you deactivate tmux be default by running <code class="language-plaintext highlighter-rouge">touch ~/.no_auto_tmux</code>), then you can create a new session by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux new <span class="nt">-s</span> session_name
</code></pre></div></div> <p>From here you can run your program. Beware that for some reason you can’t just run, say <code class="language-plaintext highlighter-rouge">python run_weight_pruning.py</code> but you have to provide the absolute path to both your Python executable and your script, which looks something like</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/venv/main/bin/python /workspace/DeepLearningLLM/run_weight_pruning.py
</code></pre></div></div> <p>To know the Python path, run this in your terminal <code class="language-plaintext highlighter-rouge">which python</code> or <code class="language-plaintext highlighter-rouge">which python3</code>.To know the path to your file, in VSCode right click on the file and click Copy Path.</p> <p>When you are disconnected from the VM for any reason, you can just SSH into the machine again and reconnect to the tmux session by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux attach <span class="nt">-t</span> session_name
</code></pre></div></div> <p>and see that whatever program you’ve start is still running or have finished without interruption.</p> <p>For more about tmux and its command, look up “tmux cheat sheet” on Google.</p> <h1 id="terminating-the-gpu-instance">Terminating the GPU instance</h1> <p>Once you’re done training your ML model, be sure to terminate the VM session otherwise vastai will charge you money. On the vastai UI there’s a Destroy instance button (self-explanatory). Next time you need to use GPU again, just rent another one and repeat this setup process.</p> <h1 id="summary">Summary</h1> <p>So that’s how I did some of my assignments this semester. As you’ve seen, renting and paying isn’t the problem, mostly it’s the setting up and getting used to the process that is most time consuming. I personally keep a bash file around so I can run it every time I need to rent a new machine, which includes all the commands I’ve listed in this post. It looks something like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># cat ~/.ssh/id_rsa_gpu.pub</span>

<span class="nb">touch</span> ~/.no_auto_tmux

<span class="nb">echo</span> <span class="s2">"🔑 Adding your private key to authorized_keys..."</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> ~/.ssh

<span class="c"># Add private key (assumes you copy this script + key to instance or SCP it)</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">' &gt; ~/.ssh/id_rsa
content_of_private_key
</span><span class="no">EOF

</span><span class="nb">chmod </span>600 ~/.ssh/id_rsa
<span class="nb">chmod </span>700 ~/.ssh

<span class="nb">eval</span> <span class="sb">`</span>ssh-agent <span class="nt">-s</span><span class="sb">`</span>
ssh-add ~/.ssh/id_rsa

<span class="c"># Avoid GitHub authenticity prompt</span>
<span class="nb">echo</span> <span class="s2">"🌐 Adding GitHub to known_hosts..."</span>
ssh-keyscan github.com <span class="o">&gt;&gt;</span> ~/.ssh/known_hosts
<span class="nb">chmod </span>644 ~/.ssh/known_hosts

<span class="c"># Optional: Configure Git identity (needed for pushing)</span>
git config <span class="nt">--global</span> user.name <span class="s2">"my name"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"my email"</span>

<span class="nb">echo</span> <span class="s2">"📂 Cloning GitHub repository..."</span>
<span class="nv">GITHUB_REPO</span><span class="o">=</span><span class="s2">"my repo link"</span>
git clone <span class="s2">"</span><span class="nv">$GITHUB_REPO</span><span class="s2">"</span>
<span class="nv">REPO_DIR</span><span class="o">=</span><span class="s2">"my repo name"</span>                     <span class="c"># Folder name after cloning</span>
<span class="nb">cd</span> <span class="s2">"</span><span class="nv">$REPO_DIR</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">"🐍 Setting up Python virtual environment..."</span>
<span class="nv">VENV_NAME</span><span class="o">=</span><span class="s2">"my virtual environment name"</span>                         <span class="c"># Virtual environment name</span>
<span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="s2">"python3.8.5"</span>               <span class="c"># Change if needed</span>
<span class="nv">$PYTHON_VERSION</span> <span class="nt">-m</span> venv <span class="s2">"</span><span class="nv">$VENV_NAME</span><span class="s2">"</span>
<span class="nb">source</span> <span class="s2">"</span><span class="nv">$VENV_NAME</span><span class="s2">/bin/activate"</span>

<span class="nb">echo</span> <span class="s2">"📦 Installing Python dependencies..."</span>
pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>]]></content><author><name></name></author><category term="study"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[Quick tips on renting GPUs to train ML models.]]></summary></entry><entry><title type="html">969 days in the life of a Data Analyst</title><link href="https://giangson19.github.io/blog/969-days-in-the-life-of-a-data-analyst/" rel="alternate" type="text/html" title="969 days in the life of a Data Analyst"/><published>2025-04-10T00:00:00+00:00</published><updated>2025-04-10T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/969-days-in-the-life-of-a-data-analyst</id><content type="html" xml:base="https://giangson19.github.io/blog/969-days-in-the-life-of-a-data-analyst/"><![CDATA[<p style="text-align: justify;">Before my Master's, I worked as a&nbsp;<strong>Data Analyst</strong> at <a href="https://onemount.com/" target="_blank" rel="noopener"><strong>One Mount Group</strong></a> (leading tech ecosystem with 3 digital products) from Oct 2021 until May 2024. I was recruited by the Talent Incubation Program (Fresh Geeks) and was mentored by <a href="https://www.linkedin.com/in/vuhoangt/">Vu Hoang</a> (now PhD Student in Information Systems, CMU). I started in the <a href="https://vinid.net/">VinID</a> analytics team (lifestyle &amp; fintech app, 13+ million users) and then worked simultaneously in the&nbsp;<a href="https://onehousing.vn/">OneHousing</a> team (proptech, 1-2 million monthly traffic) from mid 2022.</p> <center> <img style="width:50%" src="https://media.licdn.com/dms/image/v2/D5622AQHrgyBnsvx2Pw/feedshare-shrink_1280/feedshare-shrink_1280/0/1717167494624?e=1748476800&amp;v=beta&amp;t=Oo5E7MI1usjCr7oPibWOcfJr0_eAI-Y2ZRvPxWUP0sA"/> <p>My employee card. Looks very cool doesn't it?</p> </center> <p style="text-align: justify;">My work at One Mount revolved around three pillars: <strong>Business Intelligence</strong> (analytics &amp; reporting), <strong>Data Engineering&nbsp;</strong>and<strong> Machine Learning</strong>. Some tasks were unconventional for an analyst, due to my tendency to gravitate towards more technical and experimental work.</p> <p style="text-align: justify;">Below is an non-exhaustive list of the projects I contributed to along with what I did and what I learnt. I do my best to describe them without revealing sensitive information.</p> <p style="padding-left: 40px; text-align: justify;"><strong>Machine Learning</strong>:</p> <details style="padding-left: 40px;"> <summary><em>(2023) VinID Customer Income Prediction (xgboost)</em></summary> I leveraged our existing features store and experimented with XGBoost to classify customer into 3 income ranges (multi-class classification). The project was unsuccessful due to the lack of meaningful predictors, and time mismatch between labels (collected in 2019) and features (no data from 2019, so we used data from 2020-2022 as proxy).</details> <details style="padding-left: 40px;"> <summary><em>(2023) VinID Voucher attributes (decision tree)</em></summary> I fitted a decision tree on vouchers with high/low redemption rate and interpreted the tree to identify the most important attributes of a voucher that would affect its redemption.</details> <details style="padding-left: 40px;"> <summary><em>(2022) Onehousing x VinID Lookalike customers (catboost)</em></summary> I used a catboost model (binary classification) to identify customers that are similar to existing homebuyers, using features store from VinID. I also engineered some new features that was considered of high importance by the model. I learnt how to formalize business questions into data science problems, to diagnose the model's performance, and to automate steps in the machine learning pipelines to facilitate experimentations. This was also my first exposure to the imbalanced learning problem.</details> <details style="padding-left: 40px;"> <summary><em>(2022) VinID Notification Interaction Prediction (catboost)</em></summary> I used a catboost model to identify customers that are likely to interact with a notification. I also engineered some new features. I learnt how to quickly experiment with different model configurations and feature combinations.</details> <details style="padding-left: 40px;"> <summary><em>(2021) VinID Winmart holiday sales prediction (Prophet)</em></summary> I attempted to predict 2022 Tet holiday item-level sales using the Facebook's Prophet library. The model was unsuccessful due to the lack of representative data, as the 2021 data was heavily skewed by the COVID-19 pandemic. This is my first exposure to predictive analytics and time series problems.</details> <p style="padding-left: 40px; text-align: justify;">&nbsp;</p> <p style="padding-left: 40px; text-align: justify;"><strong>Analytics Projects:</strong></p> <details style="padding-left: 40px;"> <summary><em>(2024) Onehousing Customer Journey Analysis</em></summary> We analyzed the common paths (each step is a feature on the site) customers took after entering our website. We learnt that there was not a clear common path due to a lack of internal links between pages.</details> <details style="padding-left: 40px;"> <summary><em>(2023) Onehousing Non-Listing Content Problem</em></summary> We explored the behavior of organic users (i.e, they found our website via Google) and attempted to find patterns that would identify high-likelihood house buyers. I led the initiative along with two other analysts, proposed ideas to track the behavior, proposed a metric that corresponded with high retention, and did the early exploratory analysis. I also informed the data tracking template and data warehouse design for this problem.</details> <details style="padding-left: 40px;"> <summary><em>(2022) Onehousing x VinID Growth Project</em></summary> We linked customer attributes (demographic, socio-economic, spatial data, etc.) to real estate purchasing behavior to identify key customer segments. My team and I provided early insights on customer profile informing acquisition strategy, I proposed data collection and experimentation method, and built a dashboard to monitor key project metrics.</details> <p style="padding-left: 40px; text-align: justify;">&nbsp;</p> <p style="padding-left: 40px; text-align: justify;"><strong>Data Engineering</strong>:</p> <details style="padding-left: 40px;"> <summary><em>(2023) Onehousing Alert Engine (Python, SQL, dbt, Airflow)</em></summary> I designed and developed a system that automatically detects mismatched records between two data sources and sends alerts to the Operations and Sales teams. This helps significantly reduce the time spent on data reconciliation. I learnt to thinking in systems.</details> <details style="padding-left: 40px;"> <summary><em>(2023) Onehousing CEO Daily Update Bot (Python, SQL, dbt, Airflow)</em></summary> I built a script that sends daily Slack updates to the CEO about real estate deals in OneHousing. I learnt how to work with the Slack and Tableau API, as well as PyODBC.</details> <details style="padding-left: 40px;"> <summary><em>(2023) VinID Voucher, Notification, Ticketing Datamart (dimensional datawarehouse design)</em></summary> I designed and built dimensional datamarts for the various VinID business functions, which contain data about vouchers, app notification, and ticketing. I learnt a lot about dimensional modelling and data warehouse design in the process.</details> <details style="padding-left: 40px;"> <summary><em>(2022) VinID Data Platform Migration (BigQuery -&gt; Dremio)</em></summary> We changed our data platform and query engine from Bigquery to Dremio. I re-wrote and optimized SQL queries and data pipelines to fit the new platform. I learnt ELT best practices, most of my subsequent pipelines adhered to <a href="https://docs.getdbt.com/best-practices/how-we-style/0-how-we-style-our-dbt-projects" target="_blank" rel="noopener">dbt style guide</a>.</details> <details style="padding-left: 40px;"> <summary><em>All dashboards/reports/models data pipelines (SQL, dbt, Airflow)</em></summary> I built data processing pipelines (partially or entirely) for all projects that I was involved in. I learnt how write readable code and manage my code with Git.</details> <p style="padding-left: 40px; text-align: justify;">&nbsp;</p> <p style="padding-left: 40px; text-align: justify;"><strong>Dashboards:</strong></p> <details style="padding-left: 40px;"> <summary><em> (2024) Onehousing Marketing Dashboard (Power BI)</em></summary> We built executive dashboard for high-level metrics (acqured users, MAU, lead funnel, etc.) of the OneHousing website, with detailed analytical views for specific marketing functions. I learnt how to work with Power BI.&nbsp;</details> <details style="padding-left: 40px;"> <summary><em> (2023) Onehousing Online-to-Offline Dashboard (Tableau)</em></summary> I built an operational dashboard to monitor detailed lead generation and conversion activities by salespeople.</details> <details style="padding-left: 40px;"> <summary><em> (2022) VinID Ticketing Dashboard (Superset)</em></summary> I built an operational dashboard about on-app ticket sales (concerts, football matches, recreational parks etc.) and conversion funnel.</details> <details style="padding-left: 40px;"> <summary><em>(2021) VinID OneView (web-based &amp; Looker Studio)<br/></em></summary> We built a centralized business intelligence platform, containing company-wise key metrics for C-level executives (MTU, MAU, etc.). I built 2 high-level dashboards in 2021: Merchant (about voucher metrics such as claims/redeems) and Product (about north-star app metrics) and I took over as sole maintainer of all related data pipelines in 2022 (200+ tables). I learnt how to debug and track down data errors in a complex pipelines.</details>]]></content><author><name></name></author><category term="work"/><category term="data-analytics,"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[A summary of what I did in my previous industry job.]]></summary></entry><entry><title type="html">Quick introduction to Federated Learning</title><link href="https://giangson19.github.io/blog/quick-introduction-to-federated-learning/" rel="alternate" type="text/html" title="Quick introduction to Federated Learning"/><published>2025-04-05T00:00:00+00:00</published><updated>2025-04-05T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/quick-introduction-to-federated-learning</id><content type="html" xml:base="https://giangson19.github.io/blog/quick-introduction-to-federated-learning/"><![CDATA[<h1 id="centralized-machine-learning">Centralized Machine Learning</h1> <p>In traditional machine learning, all the data is stored in one place and computation all happens on one computer (roughly speaking). The machine learning problem is formalized as minimizing a loss function with respect to the weights.</p> <p>\begin{equation} \min_{w \in \mathbb{R}^d} f(w) \quad \textrm{where} \quad f(w) \overset{\text{def}}{=} \frac{1}{n} \sum_{i=1}^n f_i(w) \end{equation}</p> <ul> <li>$f_i(w)$: Loss (error) on one example, i.e., $\ell(x_i, y_i; w)$</li> <li>$f(w)$: Average loss on predictions made with parameters $w$</li> </ul> <p>And the loss function can be optimized using gradient descent. Each update is:</p> <p>\begin{equation} w_{t+1} \leftarrow w_t - \eta \nabla f(w_t) \end{equation}</p> <ul> <li>$\nabla f(w_t)$: Gradient of the loss function</li> <li>$\eta$: Learning rate</li> </ul> <p>The weights are updated iteratively until convergence or after a certain amount of times.</p> <h1 id="privacy-concerns">Privacy Concerns</h1> <p>As mentioned, to do centralized learning, all the data would need to be sent to one server for compute. This, of course, raises privacy concerns, especially if the training data contains some sensitive information about an user. For example, a keyboard suggestions model would need to collect data on what an user types.</p> <p><a href="https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/">The solution</a> proposed by Google (McMahan et al. 2016) is to distribute the data and the training. Continuing from the keyboard suggestion model, all the data would stay on the user’s phone, and the model training will happen on the device as well. They dubbed their method <strong>Federated Learning</strong>. Since its introduction by Google, some other big companies have used federated learning as well, including <a href="https://oreil.ly/IRKAw" target="_blank" rel="noopener noreferrer">Apple</a>, <a href="https://oreil.ly/YXk1C" target="_blank" rel="noopener noreferrer">NVIDIA</a>, and <a href="https://oreil.ly/e2kGc" target="_blank" rel="noopener noreferrer">Amazon Web Services</a>.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://storage.googleapis.com/gweb-research2023-media/images/1de1662a7e06d2ee886391a458652fcd-F.width-800.format-jpeg.jpg" alt="" width="75%"/> The next parts will outline how to do federated learning.</p> <h1 id="federated-learning">Federated Learning</h1> <p>Federated learning is a procedure where training takes place locally so that there is no need to send data out of the local device. In this process, there will be a centralized server (say, the Google server), and there will be K clients (say K Android users). Learning takes place as follows:</p> <ol> <li>Initialization: At the start of the process, the federated learning server will send a global model (i.e: weights) to the clients. This model could be a pre-trained keyboard suggestion model trained on some public dataset.</li> <li>Local training: The clients will perform training (i.e: gradient descent update steps) using their local data.</li> <li>Transmission: The clients will send the updated weights back to the server.</li> <li>Aggregation: The server will take the weighted average of the received models from clients to be the new global model.</li> </ol> <p>The process is repeated until convergence or after some predefined number of steps. As you can see, only the model weights are sent to the server, the data stays on the users’ phones.</p> <h1 id="local-training-with-fedsgd">Local Training with FedSGD</h1> <p>When the model is trained on a single device, we simply calculate the loss function for that client. And the loss function for the global model is the weighted average of all local losses (*). Intuitively, a client with more data points will contribute more towards the global loss.</p> <p>\begin{equation} f(w) = \sum_{k=1}^{K} \frac{n_k}{n} F_k(w) \quad \text{where} \quad F_k(w) = \frac{1}{n_k} \sum_{i \in \mathcal{P}_k} f_i(w). \end{equation}</p> <ul> <li>$F_k(w)$: Average lost for a single client</li> <li>$f(w)$: Weighted average of $F_k(w)$</li> </ul> <p>When performing gradient descent, the global weights is updated using the weighted average of the local gradients (gradients calculated on local data).</p> <p>\begin{equation} w_{t+1} \leftarrow w_t - \eta \sum_{k=1}^K \frac{n_k}{n} g_k \end{equation}</p> <ul> <li>$g_k$: Average gradient on local data $\nabla F_k(w_t)$</li> </ul> <p>This is done iteratively until… well you get the point.</p> <p>(*) This assumes that data across clients are i.i.d (independently and identically distributed). In practice, it is often the case some some client data are skewed (Zhao et al. 2018), which causes divergence problems. For possible remedy using FedProx, see Li et al. 2020.</p> <h1 id="reducing-communication-rounds-with-fedavg">Reducing Communication Rounds with FedAvg</h1> <p>In the simple FedSGD algorithm, each gradient descent step will incur one round of transmission. Imagine training a model with 100 epochs, that means all clients will have to send (and receive) model weights 100 times. This is inefficient (and for that matter, increases privacy risks that I won’t elaborate here).</p> <p>To reduce the number of communication rounds, we can perform multiple gradient descent updates locally. And afterwards, we use a weighted average of all the local models as the new global model.</p> <p>\begin{equation} w_{t+1}^{k} = w_t - \eta g_k \end{equation}</p> <p>\begin{equation} w_{t+1} \leftarrow \sum_{k=1}^K \frac{n_k}{n} w_{t+1}^{k} \end{equation}</p> <ul> <li>$w_{t+1}^{k}$: Local weight for client $k$ in iteration $t+1$</li> </ul> <p>Notice that with FedSGD, the <strong>local gradients</strong> are sent back to the server, whereas in FedAvg, the <strong>local weights</strong> (updated after several epochs) are sent instead. Moreover, the gradients are only transmitted after some number of epochs (as opposed to every epoch), thus reducing the number of communication rounds.</p> <h1 id="challenges">Challenges</h1> <p>While federated learning reduces privacy risks by keeping the data local while training the model instead of sending it to the server, it is still vulnerable to some other types of attack such as gradient inversion attack (Geiping et al. 2020) or membership inference attack. I will leave the of elaboration these risks as well as potential defenses to my future self as homework. (It literally is my homework, due in about 5 days).</p> <h1 id="references">References</h1> <p>[1] McMahan, B., Moore, E., Ramage, D., Hampson, S. &amp; Arcas, B.A.y.. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em>, in <em>Proceedings of Machine Learning Research</em> 54:1273-1282</p> <p>[2] Geiping, J., Bauermeister, H., Dröge, H., &amp; Moeller, M. (2020). Inverting gradients-how easy is it to break privacy in federated learning?. <em>Advances in neural information processing systems</em>, <em>33</em>, 16937-16947.</p> <p>[3] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., &amp; Chandra, V. (2018). Federated learning with non-iid data. <em>arXiv preprint arXiv:1806.00582</em>. </p> <p>[4] Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., &amp; Smith, V. (2020). Federated optimization in heterogeneous networks. <em>Proceedings of Machine learning and systems</em>, <em>2</em>, 429-450. </p> <p>[5] <a href="https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/" target="_blank" rel="noopener">Federated Learning: Collaborative Machine Learning without Centralized Training Data</a> </p>]]></content><author><name></name></author><category term="study"/><category term="data-science,"/><category term="machine-learning,"/><category term="data-privacy,"/><category term="deep-learning"/><summary type="html"><![CDATA[A short technical introduction to federated learning, a framework for privacy preserving machine learning.]]></summary></entry><entry><title type="html">What’s it like to study in a Master’s?</title><link href="https://giangson19.github.io/blog/whats-it-like-to-study-in-a-masters/" rel="alternate" type="text/html" title="What’s it like to study in a Master’s?"/><published>2025-03-15T00:00:00+00:00</published><updated>2025-03-15T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/whats-it-like-to-study-in-a-masters</id><content type="html" xml:base="https://giangson19.github.io/blog/whats-it-like-to-study-in-a-masters/"><![CDATA[<table> <tbody> <tr> <td>Posted by Giang Son</td> <td>Mar 15, 2025</td> <td>2 min read Quick notes on how studying in a Master’s differs from other levelsIn my humble experience, and roughly speaking.Sauce: The illustrated guide to a Ph.D.(*) which is why I think it is best to have some prior working experience before entering a Master’s. I believe you would gain a lot more compared to going in blind. Thank you for reading. I’ve also written some other posts that you can check out. Copyright © 2023 Giang Son. Meticulously handcrafted with Django and Bootstrap.</td> </tr> </tbody> </table>]]></content><author><name></name></author><summary type="html"><![CDATA[Quick notes on how studying in a Master's differs from other levels]]></summary></entry><entry><title type="html">Gentle introduction to Linear Regression</title><link href="https://giangson19.github.io/blog/gentle-introduction-to-linear-regression/" rel="alternate" type="text/html" title="Gentle introduction to Linear Regression"/><published>2025-03-01T00:00:00+00:00</published><updated>2025-03-01T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/gentle-introduction-to-linear-regression</id><content type="html" xml:base="https://giangson19.github.io/blog/gentle-introduction-to-linear-regression/"><![CDATA[<h1>The problem</h1> <p>Consider this problem: You are looking to buy a new house. You know some information about the property: the area of each floor, the number of bedrooms,&hellip; Now, you want to know how much you should be paying for that building. <em>How would you go about estimating its price?</em></p> <p>You could, in theory, go to some real estate guru and they would tell you, based on their knowledge and experience and whatnot, how much they think the house is worth. However, this manual approach may have some downsides: an expert&rsquo;s estimate could be costly, slow and biased / subjective.</p> <p>Now, suppose you already have some historical data about other houses and their prices, then you can turn to machine learning to get a cheap and <em>unbiased</em> estimate.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear_regression/lr_data_table-480.webp 480w,/assets/img/posts/linear_regression/lr_data_table-800.webp 800w,/assets/img/posts/linear_regression/lr_data_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/linear_regression/lr_data_table.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1>The data</h1> <p>(The dataset I used as example can be downloaded from Kaggle <a href="https://www.kaggle.com/datasets/lespin/house-prices-dataset">at this link</a>).</p> <p>Let&rsquo;s take a look at our dataset. The data we have is stored in a table, made up of 1460 rows and 6 columns. Each row represent a &ldquo;<em>data point</em>&rdquo; or &ldquo;<em>example</em>&rdquo; (in this case, each row is a house), and the columns are information we have on example. The column SalePrice is the value that we are trying to predict for future houses, it is usually called the &ldquo;<em>target"</em>&nbsp;or the &ldquo;<em>label</em>&rdquo;. The rest of the columns are information that we will use to make our prediction, and they are called &ldquo;<em>features</em>&rdquo;.</p> <p>For your convenience, I shall describe each of the feature below. When the data is structured (it comes in a table) and each feature has its own meaning, it is often helpful to understand the features and how it relates to our target.</p> <ul> <li><strong>OverallQual</strong>: Overall material and finish quality (from 1 to 10)</li> <li><span class="sc-jhZTHU dVJzog"><strong>BedroomAbvGr</strong>: Bedrooms above grade&nbsp;</span></li> <li><strong>GarageCars</strong>: Size of garage in car capacity</li> <li><strong>FullBath</strong>: Full bathrooms above grade</li> <li><strong>GrLivArea</strong>: Above grade (ground) living area square feet</li> </ul> <p>By machine learning convention, we usually divide the dataset into two parts: 80% of the rows will be used for training, and 20% will be used for testing. (We reserve the 20% testing set to simulate the fact that real world, our model will make predictions on data it has not seen before - we shall see how it is used to evaluate our model in a later section). We denote the number of examples used for training as \(N\), and the number of features as \(P\). In this case, \(P = 5\) and \(N = 1168\).</p> <p>Now if you squint your eyes hard enough, this table sort of looks like&hellip; a matrix. For the features, it is a matrix of \(N\) rows and \(P\) columns. For the label, it is a matrix of \(N\) rows and \(1\) column (equivalently, a column vector). We call the matrix for the features \(X\), and the label vector \(y\). (The \(y\) is in lowercase because it is a vector, whereas \(X\) is a matrix). Formally, \(X \in \mathbb{R} ^ {N \times P}\) and \(y \in \mathbb{R} ^{P}\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear_regression/lr_data_table_matrix-480.webp 480w,/assets/img/posts/linear_regression/lr_data_table_matrix-800.webp 800w,/assets/img/posts/linear_regression/lr_data_table_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/linear_regression/lr_data_table_matrix.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And here is the matrices written out explicitly:</p> <p style="text-align: center;">\( \mathbf{X} = \begin{bmatrix} 7 &amp; 3 &amp; 2 &amp; 2 &amp; 1710 \\ 6 &amp; 3 &amp; 2 &amp; 2 &amp; 1262 \\ 7 &amp; 3 &amp; 2 &amp; 2 &amp; 1786 \\ 7 &amp; 3 &amp; 3 &amp; 1 &amp; 1717 \\ 8 &amp; 4 &amp; 3 &amp; 2 &amp; 2198 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 6 &amp; 3 &amp; 2 &amp; 2 &amp; 1647 \\ 6 &amp; 3 &amp; 2 &amp; 2 &amp; 2073 \\ 7 &amp; 4 &amp; 1 &amp; 2 &amp; 2340 \\ 5 &amp; 2 &amp; 1 &amp; 1 &amp; 1078 \\ 5 &amp; 3 &amp; 1 &amp; 1 &amp; 1256 \end{bmatrix} \)</p> <p style="text-align: center;">\( y = \begin{bmatrix} 208500 \\ 181500 \\ 223500 \\ 140000 \\ 250000 \\ \vdots \\ 175000 \\ 210000 \\ 266500 \\ 142125 \\ 147500 \end{bmatrix} \)</p> <p style="text-align: left;">Writing a column vector be burdensome, so sometimes we can write one as a row vector than add a little \(^T\) (meaning "transposed") at the end, like this:&nbsp;</p> <p style="text-align: left;">\( y = [208500, 181500, ..., 142125, 147500]^T \)</p> <p>To add just a little bit more notation: we shall use superscript numbers \(^{(i)}\) to denote the \(i^{th}\) row, and the subscript number \(_m\) to denote the \(m^{th}\) column. For example, you can see that \(x^{(1)} = [7, 3, 2, 2, 1710]\) (the first row), \(x_1 = [7, 6, 7, 7, 8, &hellip;]\) (the first column OverallQual) and \(x^{(1)}_5 = 1710\).</p> <h1>The model</h1> <p>The premise of machine learning is this: there exists a model (sort of a math function) that can accurately represent the relationship between the features and the target for each training example (also for unseen data), which we shall estimate from the data. Formally: give N training examples (\(x^{(1)}, y^{(1)}\)), (\(x^{(2)}, y^{(2)}\)), &hellip; (\(x^{(N)}, y^{(N)}\)), we aim to learn a mapping \(f: x &rarr; y\) by requiring \(f(x^{(i)}) = y^{(i)}\); such that for any unseen \(x^*\)<em>, </em>\(f(x^*) = y^*\) (or something along those lines).</p> <p>We start by defining a model that represents the relationship between X and y. And by &ldquo;defining&rdquo; I mean literally guessing because a model is, more or less, our assumption of how X and y is related; and we can try out different models to see which one most accurately captures how X affects y.</p> <p>Say, we hypothesize that the price of a house has a linear relationship with the features OverallQual, BedroomAvgGr,&hellip; A linear relationship is one where when OverallQual increases by 1, then SalePrice will always increases by some fixed amount.&nbsp;</p> <p>We now have a model called <strong>Linear Regression</strong>. &ldquo;Regression&rdquo; is when our target takes continuous values (which SalePrice does), and &ldquo;Linear&rdquo; because as you see, \(y\) (SalePrice) is a linear combination of \(x\). This is the simplest model that you can come up with in machine learning.</p> <p>The functional form looks like this:</p> <p style="text-align: center;">\( f(x) = \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ...+ \beta_P \cdot x_P + \beta_0 \cdot 1 \)</p> <p>where \(\beta_0\) to \(\beta_P\) (pronounced &ldquo;beta&rdquo;) are &ldquo;learnable&rdquo; parameters (because there value will be &ldquo;learnt&rdquo; from the data, we shall see this later). \(\beta_0\) in particular is a thing called the &ldquo;bias term&rdquo; (or the &ldquo;intercept&rdquo; in statistics), but I will pretend like it&rsquo;s just another parameter like the other \(\beta\), because it is estimated in the exact same way.</p> <p>Using vector notation, we can succinctly write the model as:</p> <p style="text-align: center;">\( f(x) = X \cdot \beta \)</p> <p>where \(X\) is the feature matrix \(X = [x_1, x_2, &hellip;, x_p]\) (each \(x_m\) is a column as we have seen above) and \(\beta\) is a column vector \( \beta = [\beta_1, \beta_2, &hellip;, \beta_p]^T\). You may verify that the dot product operation \(X \cdot \beta \) indeed results in the linear combination in the previous formula.</p> <p>To add just one more bit of notation: after we have obtained \(\beta\), we can plug its values into the model and get some prediction for SalePrice, but its output will be slightly different from the true SalePrice. We call the predictions made by the model \(\hat{y}\) (pronounced y-hat). From now on, we will use the hat notation to denote anything that is estimated from data, and not ground truth data itself.</p> <p>We can write:</p> <p style="text-align: center;">\( \hat{y} = f(x) = X \cdot \beta \)</p> <p>Now, how do we train the model to learn \(\beta\)?</p> <h1>The learning</h1> <p>As we mentioned above, the objective of machine learning is to estimate a model that give an accurate approximate of some \(y^*\)<em>&nbsp;</em>when given some new data \(x^*\). In Linear Regression, we assume that the existing data and new data follow the same distribution and that the two don't affect each other's value (the "identically and independently distributed" or i.i.d assumption). In other words, we assume that when the model makes good predictions on existing data, it will also make good predictions on new data.</p> <p>How do we "train" the model to make good prediction for \(y\) based on existing data \(x\)? We can quantify the goodness of our model by measuring the error, i.e: how wrong the model is, using something called the "loss function".&nbsp; By minimizing the loss function, we get a model that is most accurate on existing data.&nbsp;</p> <p>Just bear with me for a minute to see how to minimize this and what this has to do with learning \(\beta\).</p> <p>In Linear Regression, the popular loss function to use is the Mean Squared Error (MSE):&nbsp;</p> <p style="text-align: center;">\( MSE = \frac{1}{n}\displaystyle\sum_{i=1}^N(\hat{y}^{(i)} - y^{(i)})^2 \)</p> <p>This loss function does exactly what it name says:</p> <ul> <li>for each training example, <ul> <li>measure how wrong the prediction is by subtracting the prediction from the true value, (error)</li> <li>square that up so negative difference is the same as positive difference (e.g: if the true value is 100, then a prediction of 97 is just as wrong as a prediction of 103), (squared)</li> </ul> </li> <li>take the average over all training examples (mean)</li> </ul> <p>In vector form, the loss function can be written like this: (again, feel free to verify that it is the same operation):</p> <p style="text-align: center;">\( MSE = \frac{1}{n}(\hat{y}-y)^T(\hat{y}-y) \)</p> <p>where \(\hat{y}\) and \(y\) are both column vectors, \(\hat{y} - y\) is therefore a column vector and \((\hat{y} - y)^T\) is the transpose of that vector. This dot product operation will result in squares of each element in \(\hat{y}\) and \(y\).</p> <p>Now, we can substitute \(\hat{y} = X \cdot \beta\) into this and get</p> <p style="text-align: center;">\( MSE = \frac{1}{n}(X \beta - y)^T(X\beta - y) \)</p> <p>In this function, we already know \(X\) and \(y\) (that is our given data), so the only unknown variable is \(\beta\). Our objective now is to find the values for \(\beta\) that will minimize MSE. Our machine learning problem (and most ML models) can be viewed as a function optimization problem (i.e: we are always trying to minimize some loss function).</p> <p><span style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;">This particular loss function for linear regression is convex, which we can find a closed form solution to this minimization problem. We obtain the solution by setting the partial derivative of function with respect to \(\beta\) to \(0\) and solve for \(\beta\). Formally:</span></p> <p style="text-align: center;">\( \frac{\delta \frac{1}{n}(X \beta - y)^T(X\beta - y)}{\delta{\beta}} = 0 \)</p> <p>Using vector calculus magic we can get the left hand side of the equation to the following. I will leave out the step by step details because it&rsquo;s quite complicated, so comment or DM me if you want to know how to actually derive this.</p> <p style="text-align: center;">\( X^TX\beta - X^Ty = 0 \)</p> <p>Solving for \(\beta\) we will get: (*)</p> <p style="text-align: center;">\( \beta = (X^TX)^{-1}X^Ty \) (**)</p> <p>This, ladies and gentlemen<strong>, is our machine learning model.</strong></p> <p>(*) The method we are using is equivalent to the Ordinary Least Squares method in Statistics.</p> <p>(**) The implicit implication in writing \((X^TX)^{-1}\) is that the matrix \( X^TX \) has an inverse. The informed readers may realize not all matrices are invertible. However, dealing with this is another topic I won't cover in this post.</p> <h1>Evaluation</h1> <p>That last part was quite a handful, especially if you are not too familiar with linear algebra and calculus. Don&rsquo;t worry though, in my class of over 100 people, I can see only one single guy that can follow along this particular lecture (or any lecture in Machine Learning, for that matter).</p> <p>What I want you to take away is this: our machine learning model is, in the end, obtained a bunch of matrix multiplication involving \(X\) and \(y\), which is our original data table.</p> <p>And you might ask yourself: How does that even work? Like, how does such a model have any predictive power?</p> <p>Surprisingly, it does kinda work. And even more surprisingly, lots of AI models kinda work in the same way.</p> <p>If you download the dataset and try to calculate the parameters in beta for yourself (can be done using Excel if you are not familiar with Python), then you will get the following results :</p> <p>\(\displaylines { \beta_1 = 25111.14712806\\ \beta_2 = -9149.25660938\\ \beta_3 = 21069.61456549\\ \beta_4 = 1927.11001388\\ \beta_5 = 56.44300459\\ \beta_0 =-72622.68631023428 }\)</p> <p>We can make prediction to obtain \(\hat{y}\) for the 20% we reserved in the beginning. To evaluate its accuracy, we can use the Root Mean Squared Error (the square root of the above MSE - we are using the square root because it is in the same unit as our target variable). In this case:</p> <p>\(RMSE = 41832.23667930187\)</p> <p>So when we use this model to estimate house prices, its output is, on average, about \$41k off the actual price. Considering that the average price is around \$180k, this is not completely terrible.</p> <p>We can examine this in a bit more details by plotting the Predicted Price and the Actual Price. The closer the points are to the red line, the smaller the error. If a model is 100% accurate, all the dots will be on the red line, hence error will be 0. You can see that the model does pretty well for houses worth below $350k, it is the extremely expensive houses (400k and above) that the model struggles with.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear_regression/lr_actual_vs_predicted-480.webp 480w,/assets/img/posts/linear_regression/lr_actual_vs_predicted-800.webp 800w,/assets/img/posts/linear_regression/lr_actual_vs_predicted-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/linear_regression/lr_actual_vs_predicted.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And the effectiveness of this model can be due to a few reasons:</p> <ul> <li>First, the model we chose (linear regression) is rather a decent choice for a model because lots of variables in the real world follow a linear relationship. Not all relationships are linear, but a lot.</li> <li>Second, the method that we solved for the model&rsquo;s parameters practically ensures that the predictions, on average, are as close to the real values as possible.&nbsp;</li> </ul> <p>There are ways to improve the model and drive down the error. Namely, we can choose a more complex model (say, one that can captures non-linear relationships). Or we can incorporate more features into our model (the dataset has some 80+ features, I only used 5 to keep things simple). Or better yet, we do proper feature engineering and data preparation (which is where Data Science comes in). For instance, if I use a Random Forest models along with some clever feature engineering (combining some features to create a new one), I can get the error to as low as ~23k, which is quite the improvement compare to ~41k.</p> <h1>Conclusion</h1> <p>Well well, congratulations for making it to the end of the post. By now I hope you get a taste of what machine learning is about. Even though the model we used in this post is quite simple, it does provide some predictive power, and more importantly, it provides a basis for us to learn more complex models. Essentially, a lot of machine learning modeling can be simplified to the steps that we have followed here:</p> <ul> <li>Collect some data</li> <li>Specify a model</li> <li>Define a loss function</li> <li>Find the parameters that minimize the loss function</li> </ul> <p>And even some of the math that we did here will come up again in other models, assuming that I will continue writing on machine learning. Do drop a comment if you want to see more.</p> <p>Until then, you may find the Python code for this model <a href="https://github.com/giangson19/gentle_ml/blob/main/linear_regression_housing_prices.ipynb">at this link</a> (for those unfamiliar with Python, please comment if you want some explanation).</p>]]></content><author><name></name></author><category term="study"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[A gentle introduction to the simplest machine learning problem, linear regression.]]></summary></entry><entry><title type="html">My tutoring experience</title><link href="https://giangson19.github.io/blog/my-tutoring-experience/" rel="alternate" type="text/html" title="My tutoring experience"/><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://giangson19.github.io/blog/my-tutoring-experience</id><content type="html" xml:base="https://giangson19.github.io/blog/my-tutoring-experience/"><![CDATA[<p style="text-align: justify;">From Sep to Nov 2024, I was a <strong>Data Analytics Mentor</strong> (teaching assistant) at <strong>MindX Technology School</strong>. I mentored a group of 10 adult learners in Python, SQL and Power BI lessons. I expanded on concepts taught by the main teacher, guided students through technical exercises and provided them with practical advice on pursuing a career in data.&nbsp;</p> <p style="text-align: justify;">Between Dec 2022 and May 2024, I was a <strong>Data Analytics Mentor</strong> (internal trainer) at <strong>One Mount</strong>. I built a custom curriculum and taught a 5-lesson introductory SQL course for 30+ colleagues from the Marketing and Business Development department in Dec 2022. I gave practical advice on querying large datasets (billions of rows) and analytical methods specific to our business context.</p> <center><img src="https://media.licdn.com/dms/image/v2/D5622AQHiCYDSnAAw3Q/feedshare-shrink_800/feedshare-shrink_800/0/1717167494327?e=1748476800&amp;v=beta&amp;t=kwEyRJsasOTn7NBTStZfkjZhLauHeB_w26-bBSDy6jk" style="width:30%;"/></center> <p></p> <p style="text-align: justify;">Within my data analytics team of 10+ (our population varied over the years), I led multiple training sessions on technical skills relevant to our projects: applied machine learning, machine learning explanability, inferential statistics, dimensional data warehouse design (most of which I self-learnt). Ocasionally, I shared my knowledge about soft skills, namely critical thinking and problem solving.</p> <p style="text-align: justify;">From Mar to Nov 2019, I was an <strong>IELTS Tutor</strong> (teaching assistant) at <strong>LinhUK IELTS School</strong>. I assisted in classes of 4 IELTS skills, working mostly with high-school and university students. I graded writing assignments and provided detailed feedback on how to improve grammatical structure, lexical usage, and idea development. I also trained individually with numerous students in speaking exercise, giving practical advice on pronounciation, intonation and building reflexes to test questions.</p>]]></content><author><name></name></author><category term="work"/><category term="data-analytics,"/><category term="data-science,"/><category term="machine-learning"/><summary type="html"><![CDATA[Looking back at the times when I shared my knowledge]]></summary></entry></feed>