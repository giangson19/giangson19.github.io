<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Quick introduction to Federated Learning | Nguyen Giang Son </title> <meta name="author" content="Nguyen Giang Son"> <meta name="description" content="A short technical introduction to federated learning, a framework for privacy preserving machine learning."> <meta name="keywords" content="data-science, artificial-intelligence, multimodal-ai, research, nlp"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?bfe4ed2966fee633b02bec33efefc1bf"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://giangson19.github.io/blog/quick-introduction-to-federated-learning/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Nguyen Giang <span class="font-weight-bold">Son</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Quick introduction to Federated Learning</h1> <p class="post-meta"> Created on April 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> data-science,</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning,</a>   <a href="/blog/tag/data-privacy"> <i class="fa-solid fa-hashtag fa-sm"></i> data-privacy,</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   ·   <a href="/blog/category/study"> <i class="fa-solid fa-tag fa-sm"></i> study</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"><a href="#centralized-machine-learning">Centralized Machine Learning</a></li> <li class="toc-entry toc-h1"><a href="#privacy-concerns">Privacy Concerns</a></li> <li class="toc-entry toc-h1"><a href="#federated-learning">Federated Learning</a></li> <li class="toc-entry toc-h1"><a href="#local-training-with-fedsgd">Local Training with FedSGD</a></li> <li class="toc-entry toc-h1"><a href="#reducing-communication-rounds-with-fedavg">Reducing Communication Rounds with FedAvg</a></li> <li class="toc-entry toc-h1"><a href="#challenges">Challenges</a></li> <li class="toc-entry toc-h1"><a href="#references">References</a></li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="centralized-machine-learning">Centralized Machine Learning</h1> <p>In traditional machine learning, all the data is stored in one place and computation all happens on one computer (roughly speaking). The machine learning problem is formalized as minimizing a loss function with respect to the weights.</p> <p>\begin{equation} \min_{w \in \mathbb{R}^d} f(w) \quad \textrm{where} \quad f(w) \overset{\text{def}}{=} \frac{1}{n} \sum_{i=1}^n f_i(w) \end{equation}</p> <ul> <li>$f_i(w)$: Loss (error) on one example, i.e., $\ell(x_i, y_i; w)$</li> <li>$f(w)$: Average loss on predictions made with parameters $w$</li> </ul> <p>And the loss function can be optimized using gradient descent. Each update is:</p> <p>\begin{equation} w_{t+1} \leftarrow w_t - \eta \nabla f(w_t) \end{equation}</p> <ul> <li>$\nabla f(w_t)$: Gradient of the loss function</li> <li>$\eta$: Learning rate</li> </ul> <p>The weights are updated iteratively until convergence or after a certain amount of times.</p> <h1 id="privacy-concerns">Privacy Concerns</h1> <p>As mentioned, to do centralized learning, all the data would need to be sent to one server for compute. This, of course, raises privacy concerns, especially if the training data contains some sensitive information about an user. For example, a keyboard suggestions model would need to collect data on what an user types.</p> <p><a href="https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/" rel="external nofollow noopener" target="_blank">The solution</a> proposed by Google (McMahan et al. 2016) is to distribute the data and the training. Continuing from the keyboard suggestion model, all the data would stay on the user’s phone, and the model training will happen on the device as well. They dubbed their method <strong>Federated Learning</strong>. Since its introduction by Google, some other big companies have used federated learning as well, including <a href="https://oreil.ly/IRKAw" target="_blank" rel="noopener noreferrer">Apple</a>, <a href="https://oreil.ly/YXk1C" target="_blank" rel="noopener noreferrer">NVIDIA</a>, and <a href="https://oreil.ly/e2kGc" target="_blank" rel="noopener noreferrer">Amazon Web Services</a>.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://storage.googleapis.com/gweb-research2023-media/images/1de1662a7e06d2ee886391a458652fcd-F.width-800.format-jpeg.jpg" alt="" width="75%"> The next parts will outline how to do federated learning.</p> <h1 id="federated-learning">Federated Learning</h1> <p>Federated learning is a procedure where training takes place locally so that there is no need to send data out of the local device. In this process, there will be a centralized server (say, the Google server), and there will be K clients (say K Android users). Learning takes place as follows:</p> <ol> <li>Initialization: At the start of the process, the federated learning server will send a global model (i.e: weights) to the clients. This model could be a pre-trained keyboard suggestion model trained on some public dataset.</li> <li>Local training: The clients will perform training (i.e: gradient descent update steps) using their local data.</li> <li>Transmission: The clients will send the updated weights back to the server.</li> <li>Aggregation: The server will take the weighted average of the received models from clients to be the new global model.</li> </ol> <p>The process is repeated until convergence or after some predefined number of steps. As you can see, only the model weights are sent to the server, the data stays on the users’ phones.</p> <h1 id="local-training-with-fedsgd">Local Training with FedSGD</h1> <p>When the model is trained on a single device, we simply calculate the loss function for that client. And the loss function for the global model is the weighted average of all local losses (*). Intuitively, a client with more data points will contribute more towards the global loss.</p> <p>\begin{equation} f(w) = \sum_{k=1}^{K} \frac{n_k}{n} F_k(w) \quad \text{where} \quad F_k(w) = \frac{1}{n_k} \sum_{i \in \mathcal{P}_k} f_i(w). \end{equation}</p> <ul> <li>$F_k(w)$: Average lost for a single client</li> <li>$f(w)$: Weighted average of $F_k(w)$</li> </ul> <p>When performing gradient descent, the global weights is updated using the weighted average of the local gradients (gradients calculated on local data).</p> <p>\begin{equation} w_{t+1} \leftarrow w_t - \eta \sum_{k=1}^K \frac{n_k}{n} g_k \end{equation}</p> <ul> <li>$g_k$: Average gradient on local data $\nabla F_k(w_t)$</li> </ul> <p>This is done iteratively until… well you get the point.</p> <p>(*) This assumes that data across clients are i.i.d (independently and identically distributed). In practice, it is often the case some some client data are skewed (Zhao et al. 2018), which causes divergence problems. For possible remedy using FedProx, see Li et al. 2020.</p> <h1 id="reducing-communication-rounds-with-fedavg">Reducing Communication Rounds with FedAvg</h1> <p>In the simple FedSGD algorithm, each gradient descent step will incur one round of transmission. Imagine training a model with 100 epochs, that means all clients will have to send (and receive) model weights 100 times. This is inefficient (and for that matter, increases privacy risks that I won’t elaborate here).</p> <p>To reduce the number of communication rounds, we can perform multiple gradient descent updates locally. And afterwards, we use a weighted average of all the local models as the new global model.</p> <p>\begin{equation} w_{t+1}^{k} = w_t - \eta g_k \end{equation}</p> <p>\begin{equation} w_{t+1} \leftarrow \sum_{k=1}^K \frac{n_k}{n} w_{t+1}^{k} \end{equation}</p> <ul> <li>$w_{t+1}^{k}$: Local weight for client $k$ in iteration $t+1$</li> </ul> <p>Notice that with FedSGD, the <strong>local gradients</strong> are sent back to the server, whereas in FedAvg, the <strong>local weights</strong> (updated after several epochs) are sent instead. Moreover, the gradients are only transmitted after some number of epochs (as opposed to every epoch), thus reducing the number of communication rounds.</p> <h1 id="challenges">Challenges</h1> <p>While federated learning reduces privacy risks by keeping the data local while training the model instead of sending it to the server, it is still vulnerable to some other types of attack such as gradient inversion attack (Geiping et al. 2020) or membership inference attack. I will leave the of elaboration these risks as well as potential defenses to my future self as homework. (It literally is my homework, due in about 5 days).</p> <h1 id="references">References</h1> <p>[1] McMahan, B., Moore, E., Ramage, D., Hampson, S. &amp; Arcas, B.A.y.. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em>, in <em>Proceedings of Machine Learning Research</em> 54:1273-1282</p> <p>[2] Geiping, J., Bauermeister, H., Dröge, H., &amp; Moeller, M. (2020). Inverting gradients-how easy is it to break privacy in federated learning?. <em>Advances in neural information processing systems</em>, <em>33</em>, 16937-16947.</p> <p>[3] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., &amp; Chandra, V. (2018). Federated learning with non-iid data. <em>arXiv preprint arXiv:1806.00582</em>. </p> <p>[4] Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., &amp; Smith, V. (2020). Federated optimization in heterogeneous networks. <em>Proceedings of Machine learning and systems</em>, <em>2</em>, 429-450. </p> <p>[5] <a href="https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/" target="_blank" rel="noopener">Federated Learning: Collaborative Machine Learning without Centralized Training Data</a> </p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://giangson.me/blog/whats-it-like-to-study-in-a-masters/" target="_blank" rel="external nofollow noopener">What’s it like to study in a Master’s?</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/969-days-in-the-life-of-a-data-analyst/">969 days in the life of a data analyst</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/gentle-introduction-to-linear-regression/">Gentle introduction to Linear Regression</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'giangson19/giangson19.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Nguyen Giang Son. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>